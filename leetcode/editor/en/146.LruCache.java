import java.util.*;
import java.util.concurrent.atomic.AtomicLong;

/**
 * Design a data structure that follows the constraints of a Least Recently Used (
 * LRU) cache.
 * <p>
 * Implement the LRUCache class:
 * <p>
 * <p>
 * LRUCache(int capacity) Initialize the LRU cache with positive size capacity.
 * int get(int key) Return the value of the key if the key exists, otherwise
 * return -1.
 * void put(int key, int value) Update the value of the key if the key exists.
 * Otherwise, add the key-value pair to the cache. If the number of keys exceeds the
 * capacity from this operation, evict the least recently used key.
 * <p>
 * <p>
 * The functions get and put must each run in O(1) average time complexity.
 * <p>
 * <p>
 * Example 1:
 * <p>
 * <p>
 * Input
 * ["LRUCache", "put", "put", "get", "put", "get", "put", "get", "get", "get"]
 * [[2], [1, 1], [2, 2], [1], [3, 3], [2], [4, 4], [1], [3], [4]]
 * Output
 * [null, null, null, 1, null, -1, null, -1, 3, 4]
 * <p>
 * Explanation
 * LRUCache lRUCache = new LRUCache(2);
 * lRUCache.put(1, 1); // cache is {1=1}
 * lRUCache.put(2, 2); // cache is {1=1, 2=2}
 * lRUCache.get(1);    // return 1
 * lRUCache.put(3, 3); // LRU key was 2, evicts key 2, cache is {1=1, 3=3}
 * lRUCache.get(2);    // returns -1 (not found)
 * lRUCache.put(4, 4); // LRU key was 1, evicts key 1, cache is {4=4, 3=3}
 * lRUCache.get(1);    // return -1 (not found)
 * lRUCache.get(3);    // return 3
 * lRUCache.get(4);    // return 4
 * <p>
 * <p>
 * <p>
 * Constraints:
 * <p>
 * <p>
 * 1 <= capacity <= 3000
 * 0 <= key <= 10â´
 * 0 <= value <= 10âµ
 * At most 2 * 10âµ calls will be made to get and put.
 * <p>
 * <p>
 * Related Topics Hash Table Linked List Design Doubly-Linked List ğŸ‘ 20844 ğŸ‘ 105
 * 0
 */
       
/*
 2024-08-21 18:06:23
 LRU Cache
Category	Difficulty	Likes	Dislikes
algorithms	Medium (42.43%)	20844	1050
Tags
design

Companies
amazon | bloomberg | facebook | google | microsoft | palantir | snapchat | twitter | uber | yahoo | zenefits
*/

class LruCache {
    public static void main(String[] args) {
//        Solution solution = new LruCache().new Solution();
    }

    //leetcode submit region begin(Prohibit modification and deletion)
    class LRUCache {
        //https://www.youtube.com/watch?v=q0UrgXS5lnk&list=PLbaIOC0vpjNU8ldi-uUxkQ2GSeZm4wf7D&index=11
        //æ—¶é—´çª—å£æ˜ å°„Timed Window Map
        //LRUåŸºäºè®¿é—®æ—¶é—´ï¼ŒLRUé€‚åˆçªå‘æ€§çš„ã€å‘¨æœŸæ€§çš„æ‰¹é‡æ“ä½œ
        //Hashmap å­˜<key, Node> å­˜å„²ç·©å­˜ å¯¦ç¾ get,put o(1)æ“åš
        //Double Linked Node é›™éŠéŒ¶ ç¶­è­·Nodeçš„ä½¿ç”¨çš„æ–°èˆŠï¼Œå¾headåˆ°tail,è¡¨ç¤ºå¾æœ€ä¹…ä½¿ç”¨åˆ°æœ€è¿‘ä½¿ç”¨
        class Node {
            int val;
            int key;
            Node pre;
            Node next;

            public Node(int key, int val) {
                this.val = val;
                this.key = key;
            }
        }

        Map<Integer, Node> cache;
        //è¡¨ç¤ºé€£æ¥æœ€ä¹…ä½¿ç”¨çš„ç¯€é»
        Node head;
        //è¡¨ç¤ºé€£æ¥æœ€è¿‘ä½¿ç”¨çš„ç¯€é»
        Node tail;
        int size;
        int capacity;

        //åˆå§‹åŒ–LRUCache
        public LRUCache(int capacity) {
            this.capacity = capacity;
            cache = new HashMap<>();
            head = new Node(-1, -1);
            tail = new Node(-1, -1);
            head.next = tail;
            tail.pre = head;
        }

        public int get(int key) {
            if (!cache.containsKey(key)) {
                return -1;
            }
            Node cur = cache.get(key);
            //æ›´æ–°å½“å‰èŠ‚ç‚¹ä¸ºæœ€æ–°ä½¿ç”¨ï¼Œå³ç§»å‹•åˆ°ç·´è¡¨æœ«å°¾:
            //1.æ–·é–‹è¡¨ç¤ºç•¶å‰çš„ç¯€é»ä½ç½®
            cur.pre.next = cur.next;
            cur.next.pre = cur.pre;
            //2.ç§»å‹•åˆ°æœ€è¿‘ä½¿ç”¨ç¯€é»
            moveToTail(cur);
            return cur.val;
        }

        //ç•¶å‰ä½¿ç”¨çš„ç¯€é»æ’å…¥åˆ°tailç¯€é»å‰
        private void moveToTail(Node cur) {
            cur.next = tail;
            cur.pre = tail.pre;
            tail.pre.next = cur;
            tail.pre = cur;
        }

        public void put(int key, int value) {
            if (cache.containsKey(key)) {
                Node existNode = cache.get(key);
                existNode.val = value;
                //å¾©ç”¨get
                get(key);
                return;
            }
            //cacheå®¹é‡æ»¿ï¼Œevictæœ€å°‘ä½¿ç”¨node
            if (size == capacity) {
                Node lruNode = head.next;
                //ç§»é™¤lruNodeç¯€é»çš„é »ç‡ç´€éŒ„
                lruNode.next.pre = lruNode.pre;
                lruNode.pre.next = lruNode.next;
                //ç§»é™¤ç¯€é»
                cache.remove(lruNode.key);
                size--;
            }
            //å¯«å…¥cache
            Node newNode = new Node(key, value);
            cache.put(newNode.key, newNode);
            size++;
            //ç´€éŒ„é »ç‡
            moveToTail(newNode);
        }
    }
    /**
     * Your LRUCache object will be instantiated and called as such:
     * LRUCache obj = new LRUCache(capacity);
     * int param_1 = obj.get(key);
     * obj.put(key,value);
     */


    /**
     * WindowedMap:(LRUå˜ç§ ä»å°¾éƒ¨æ’å…¥ï¼Œ ä»å¤´éƒ¨å–å‡ºæœ€æ—©æ’å…¥çš„å…ƒç´ åˆ¤æ–­æ˜¯å¦å¤±æ•ˆ)
     * IMplement WindowedMap: key:value store , where entries get expired after certain time EXPIRY_TIME (pre configured value)
     * three API calls
     * get(key)
     * put(key, val)
     * get_average() -> average of Yall the values (non-expired)
     * The data would be streaming in increasing timestamp
     * They put heavy focus on concurrency ,
     * <p>
     * LinkedHashMap +
     */

    class WindowedMap<K, V extends Number> {
        //å­˜å‚¨é”®å€¼å¯¹æ—¶é—´æˆ³
        class Node<V> {
            V val;
            Long timesstamp;

            Node(V val, Long timesstamp) {
                this.val = val;
                this.timesstamp = timesstamp;
            }
        }
        //è¿‡æœŸæ—¶é—´
        private final long EXPIRY_TIEM;
        //æŒ‰æ’å…¥é¡ºåºå­˜å‚¨é”®å€¼å¯¹ï¼Œ    //å› ä¸ºé“¾è¡¨æ˜¯æŒ‰ç…§æ’å…¥é¡ºåºæ’åºçš„ï¼Œå¤´éƒ¨ä¸ºæœ€è€èŠ‚ç‚¹ï¼Œå°¾éƒ¨ä¸ºæœ€æ–°èŠ‚ç‚¹ï¼Œæ‰€ä»¥å¤´éƒ¨ä¹‹åçš„æ¡ç›®ä¸ä¼šè¿‡æœŸ
        private final LinkedHashMap<K, Node<V>> linkedHashMap;
        private final AtomicLong totalVal = new AtomicLong(0);
        private final AtomicLong count = new AtomicLong(0);

        public WindowedMap(Long expiryTime) {
            EXPIRY_TIEM = expiryTime;
            // LinkedHashMap with access order set to false (insertion order)
            this.linkedHashMap = new LinkedHashMap<K, Node<V>>(16, 0.75f, false) {
                protected boolean removeEldestEntry(Map.Entry<K, Node<V>> eldest) {
                    // Automatically remove expired entries based on expiry time
                    return (System.currentTimeMillis() - eldest.getValue().timesstamp > EXPIRY_TIEM);
                }
            };
        }


        //æ’å…¥é”®å€¼å¯¹
        public synchronized void put(K key, V val){
            long now = System.currentTimeMillis();
            Node<V> oldNode = linkedHashMap.remove(key);
            //å¦‚æœéµå­˜åœ¨ï¼Œåˆ™å…ˆç§»é™¤ï¼Œæ›´æ–°totalValï¼Œcount
            if(null != oldNode){
                totalVal.addAndGet(-oldNode.val.longValue());
                count.decrementAndGet();
            }
            //æ’å…¥æ–°èŠ‚ç‚¹åˆ°é˜Ÿåˆ—å°¾éƒ¨ //æ³¨æ„ï¼šcompile error linkedHashMap.put(key,val);
            linkedHashMap.put(key, new Node<>(val, now));
            totalVal.addAndGet(val.longValue());
            count.incrementAndGet();
            //å¤„ç†è¿‡æœŸæ¡ç›®
            cleanUpExpiredEntries();

        }

        //è·å–keyå¯¹åº”çš„val
        public synchronized V get(K key){
            //å¤„ç†è¿‡æœŸæ¡ç›®
            cleanUpExpiredEntries();

            Node<V> node = linkedHashMap.get(key);
            return node == null? null : node.val;
        }

        //è®¡ç®—average
        public synchronized double getAverage(){
            //å¤„ç†è¿‡æœŸæ¡ç›®
            cleanUpExpiredEntries();

            return count.get() == 0
                    ? 0.0
                    : (double) totalVal.longValue()/count.longValue();
        }

        public void  cleanUpExpiredEntries(){
            long currentTime = System.currentTimeMillis();
            Iterator<Map.Entry<K, Node<V>>> iterator = linkedHashMap.entrySet().iterator();

            while (iterator.hasNext()){
                Map.Entry<K, Node<V>> entry = iterator.next();
                Node<V> node = entry.getValue();

                if((currentTime -node.timesstamp)> EXPIRY_TIEM){
                    iterator.remove();
                    totalVal.addAndGet(-node.val.longValue());
                    count.decrementAndGet();
                } else {
                    //å› ä¸ºé“¾è¡¨æ˜¯æŒ‰ç…§æ’å…¥é¡ºåºæ’åºçš„ï¼Œå¤´éƒ¨ä¸ºæœ€è€èŠ‚ç‚¹ï¼Œå°¾éƒ¨ä¸ºæœ€æ–°èŠ‚ç‚¹ï¼Œæ‰€ä»¥å¤´éƒ¨ä¹‹åçš„æ¡ç›®ä¸ä¼šè¿‡æœŸ
                    break;
                }

            }
        }
    }

    /** confluent coding
     * You are given a list of documents with id and text.
     * Eg :-
     * DocId, Text
     * 1, "Cloud computing is the on-demand availability of computer system resources."
     * 2, "One integrated service for metrics uptime cloud monitoring dashboards and alerts reduces time spent navigating between systems."
     * 3, "Monitor entire cloud infrastructure, whether in the cloud computing is or in virtualized data centers."
     *
     * Search a given phrase in all the documents in a efficient manner. Assume that you have more than 1 million docs.
     * Eg :-
     * search("cloud") >> This should output [1,2,3]
     * search("cloud monitoring") >> This should output [2]
     * search("Cloud computing is") >> This should output [1,3]
     * ä¸ºäº†åœ¨è¶…è¿‡100ä¸‡ä»½æ–‡æ¡£ä¸­é«˜æ•ˆåœ°æœç´¢å•è¯ / çŸ­è¯­ï¼Œä¸€ä¸ªå¸¸è§çš„æ–¹æ³•æ˜¯é¢„å¤„ç†æ•°æ®ï¼Œæ„å»ºå€’æ’ç´¢å¼•ï¼ˆæœç´¢å¼•æ“ä¸­å¸¸ç”¨çš„æŠ€æœ¯ï¼‰ã€‚å€’æ’ç´¢å¼•å°†è¯æˆ–çŸ­è¯­æ˜ å°„åˆ°åŒ…å«å®ƒä»¬çš„æ–‡æ¡£IDä¸Šï¼Œä»è€Œåœ¨æœç´¢æ—¶å¯ä»¥å¿«é€Ÿæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‚
     *
     * ä»¥ä¸‹æ˜¯ä½¿ç”¨HashMapå®ç°å€’æ’ç´¢å¼•çš„è§£å†³æ–¹æ¡ˆï¼Œå…¶ä¸­é”®æ˜¯å•è¯æˆ–çŸ­è¯­ï¼Œå€¼æ˜¯åŒ…å«è¯¥å•è¯æˆ–çŸ­è¯­çš„æ–‡æ¡£IDé›†åˆã€‚é¢„å¤„ç†æ­¥éª¤æ„å»ºç´¢å¼•ï¼Œæœç´¢æŸ¥è¯¢é€šè¿‡ç´¢å¼•å¿«é€ŸæŸ¥æ‰¾ç›¸å…³æ–‡æ¡£ã€‚
     * */

    class DocumentWordSearch{
        // å€’æ’ç´¢å¼•ï¼šç”¨äºå­˜å‚¨å•è¯æˆ–çŸ­è¯­åˆ°æ–‡æ¡£IDçš„æ˜ å°„
        // Inverted index to store word -> document IDs mapping
        private final Map<String, Set<Integer>>invertedIndex = new HashMap<>();


        // Method to add documents to the inverted index
        public void addDocument(int docId, String text){
            String[] words  = text.toLowerCase().split("\\s+");
            for (int i = 0; i< words.length;i++){
                invertedIndex.computeIfAbsent(words[i], k-> new HashSet<>()).add(docId);
            }
        }

        // Method to search for a phrase and return matching document IDs
        public List<Integer> search(String query){
            String normalizedQuery = query.toLowerCase();
            Set<Integer> docIds = invertedIndex.getOrDefault(normalizedQuery, Collections.emptySet());
            return new ArrayList<>(docIds);
        }
    }

    class DocumentPhraseSearch{
        // Inverted index to store phrase -> document IDs mapping
        private final Map<String, Set<Integer>>invertedIndex = new HashMap<>();

        // Method to add documents to the inverted index
        public void addDocument(int docId, String text){
            //Tokenize the text into words
            String[] words= text.toLowerCase().split("\\s+");
            for (int i=0; i< words.length; i++){
                //// Build phrases by progressively adding more words
                for (int j =i ;j<words.length ; j++){
                    String phrase = String.join(" ",Arrays.copyOfRange(words,i,j+1));
                    //// Add the phrase to the inverted index
                    invertedIndex.computeIfAbsent(phrase, k -> new HashSet<>()).add(docId);
                }
            }
        }

        // Method to search for a phrase and return matching document IDs
        public List<Integer> search(String query){
            String normalizedQuery = query.toLowerCase();
            Set<Integer> docIds = invertedIndex.getOrDefault(normalizedQuery, Collections.emptySet());
            return new ArrayList<>(docIds);
        }
    }


    /**
     * isVariadic
     *
     * */

    class Function {
        String name;
        List<String> argumentTypes;
        boolean isVariadic;

        Function(String name, List<String> argumentTypes, boolean isVariadic) {
            this.name = name;
            this.argumentTypes = argumentTypes;
            this.isVariadic = isVariadic;
        }

        @Override
        public String toString() {
            return name;
        }
    }

    class FunctionLibrary {
        private Map<String, List<Function>> functionMap = new HashMap<>();  // ä½¿ç”¨HashMapä¼˜åŒ–

        // æ³¨å†Œå‡½æ•°æ–¹æ³•
        public void register(Set<Function> functions) {
            for (Function function : functions) {
                String key = generateKey(function.argumentTypes, function.isVariadic); // ç”Ÿæˆå”¯ä¸€é”®
                functionMap.computeIfAbsent(key, k -> new ArrayList<>())
                        .add(function);  // å°†å‡½æ•°æ·»åŠ åˆ°Map
            }
        }

        // æŸ¥æ‰¾åŒ¹é…çš„å‡½æ•°æ–¹æ³•
        public List<Function> findMatches(List<String> inputArgumentTypes) {
            List<Function> matches = new ArrayList<>();
            String key = generateKey(inputArgumentTypes, false);

            // æ£€æŸ¥ç²¾ç¡®åŒ¹é…çš„å‡½æ•°
            if (functionMap.containsKey(key)) {
                matches.addAll(functionMap.get(key));
            }

            // æ£€æŸ¥å¯å˜å‚æ•°åŒ¹é…çš„å‡½æ•°
            for (Map.Entry<String, List<Function>> entry : functionMap.entrySet()) {
                String functionKey = entry.getKey();
                if (isVariadicKey(functionKey) && matchesVariadic(inputArgumentTypes, functionKey)) {
                    matches.addAll(entry.getValue());
                }
            }
            return matches;
        }

        // æ ¹æ®å‚æ•°ç±»å‹åˆ—è¡¨ç”Ÿæˆå”¯ä¸€é”®
        private String generateKey(List<String> argumentTypes, boolean isVariadic) {
            StringBuilder keyBuilder = new StringBuilder();
            for (String arg : argumentTypes) {
                keyBuilder.append(arg).append(",");
            }
            if (isVariadic) {
                keyBuilder.append("...");  // æ ‡è¯†å¯å˜å‚æ•°
            }
            return keyBuilder.toString();
        }

        // åˆ¤æ–­ä¸€ä¸ªé”®æ˜¯å¦ä¸ºå¯å˜å‚æ•°çš„é”®
        private boolean isVariadicKey(String key) {
            return key.endsWith("...");
        }

        // æ£€æŸ¥å¯å˜å‚æ•°å‡½æ•°æ˜¯å¦åŒ¹é…
        private boolean matchesVariadic(List<String> inputArgumentTypes, String functionKey) {
            //è¿™é‡Œå‡å»3æ˜¯å› ä¸ºå¯å˜å‚æ•°çš„é”®ä»¥"..."ç»“å°¾ã€‚é€šè¿‡å»æ‰æœ€å3ä¸ªå­—ç¬¦ï¼Œæˆ‘ä»¬ç§»é™¤äº†è¡¨ç¤ºå¯å˜å‚æ•°çš„"..."ã€‚
            // ç§»é™¤ç»“å°¾çš„ "..."
            String argsString = functionKey.substring(0, functionKey.length() - 3);
            String[] fixedArgs = argsString.split(",");

            // å¯å˜å‚æ•°ç±»å‹æ˜¯æœ€åä¸€ä¸ªå›ºå®šå‚æ•°çš„ç±»å‹
            String variadicType = fixedArgs[fixedArgs.length - 1].trim();

            // æ£€æŸ¥å›ºå®šå‚æ•°æ•°é‡ï¼ˆä¸åŒ…æ‹¬å¯å˜å‚æ•°ï¼‰
            if (fixedArgs.length - 1 > inputArgumentTypes.size()) {
                return false;
            }

            // æ£€æŸ¥å›ºå®šå‚æ•°ç±»å‹åŒ¹é…ï¼ˆé™¤äº†æœ€åä¸€ä¸ªï¼Œå› ä¸ºæœ€åä¸€ä¸ªæ˜¯å¯å˜å‚æ•°ç±»å‹ï¼‰
            for (int i = 0; i < fixedArgs.length - 1; i++) {
                if (!fixedArgs[i].trim().equals(inputArgumentTypes.get(i))) {
                    return false;
                }
            }

            // æ£€æŸ¥å¯å˜å‚æ•°éƒ¨åˆ†
            for (int i = fixedArgs.length - 1; i < inputArgumentTypes.size(); i++) {
                if (!variadicType.equals(inputArgumentTypes.get(i))) {
                    return false;
                }
            }
            return true;
        }
    }

//    public class Main {
//        public static void main(String[] args) {
//            Function funcA = new Function("FuncA", Arrays.asList("String", "Integer", "Integer"), false);
//            Function funcB = new Function("FuncB", Arrays.asList("String", "Integer"), true);
//            Function funcC = new Function("FuncC", Arrays.asList("Integer"), true);
//            Function funcD = new Function("FuncD", Arrays.asList("Integer", "Integer"), true);
//            Function funcE = new Function("FuncE", Arrays.asList("Integer", "Integer", "Integer"), false);
//            Function funcF = new Function("FuncF", Arrays.asList("String"), false);
//            Function funcG = new Function("FuncG", Arrays.asList("Integer"), false);
//
//            FunctionLibrary library = new FunctionLibrary();
//            Set<Function> functions = new HashSet<>(Arrays.asList(funcA, funcB, funcC, funcD, funcE, funcF, funcG));
//            library.register(functions);
//
//            System.out.println(library.findMatches(Arrays.asList("String"))); // [FuncF]
//            System.out.println(library.findMatches(Arrays.asList("Integer"))); // [FuncC, FuncG]
//            System.out.println(library.findMatches(Arrays.asList("Integer", "Integer", "Integer", "Integer"))); // [FuncC, FuncD]
//            System.out.println(library.findMatches(Arrays.asList("Integer", "Integer", "Integer"))); // [FuncC, FuncD, FuncE]
//            System.out.println(library.findMatches(Arrays.asList("String", "Integer", "Integer", "Integer"))); // [FuncB]
//            System.out.println(library.findMatches(Arrays.asList("String", "Integer", "Integer"))); // [FuncA, FuncB]
//        }
//    }


//leetcode submit region end(Prohibit modification and deletion)

}